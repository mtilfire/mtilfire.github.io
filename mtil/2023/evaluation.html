<!DOCTYPE html>
<html lang="en">
<head>
	<title>MTIL</title>
	<meta charset="utf-8">
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

	
	<style>
		/* Remove the navbar's default margin-bottom and rounded borders */
		.navbar {
			padding: 10px;
			margin-bottom: 0px;
			border-radius: 0px;
			font-size: 20px;
			text-decoration: none;
		}

		/* Add a gray background color and some padding to the footer */
		footer {
			background-color: #f2f2f2;
			padding: 25px;
			position: relative;
			bottom: 0;
			width: 100%;
		}

		p {
			font-size: 16px;
		}


		.navbar-nav .dropdown .dropdown-menu :hover {
    		background-color: transparent;

  		}
  		
  		
			
	</style>
</head>
<body>

<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
  <a class="navbar-brand" href="index.html">MTIL</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
     <li class="nav-item">
        <a class="nav-link" href="index.html">Introduction<span class="sr-only">(current)</span></a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="overview_of_task.html">Overview of Task</a>
      </li>
       <li class="nav-item ">
        <a class="nav-link active" href="evaluation.html">Evaluation</a>
      </li>
     
        <li class="nav-item ">
        <a class="nav-link" href="dataset.html">Datasets</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-link" href="registration.html">Registration</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="important_dates.html">Important dates</a>
      </li>
      <li class="nav-item ">
        <a class="nav-link" href="organizers.html">Organizers</a>
      </li>
     
      <!-- <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
          Archives
        </a>
        <div class="dropdown-menu bg-dark" aria-labelledby="navbarDropdown">
          <a class="dropdown-item  text-white" href="../archives/datasets.html">Datasets</a>
          <a class="dropdown-item  text-white" href="../archives/proceedings.html">Past Proceedings</a>
          <a class="dropdown-item  text-white" href="../2020/index.html">EDNIL 2020</a>
        </div>
      </li>
       -->
    </ul>
  </div>
</nav>

<div class="jumbotron">
	<div class="container text-center">
		<h1>MTIL (2023)</h1>
		<p>Machine Translation for Indian Languages</p>
	</div>
</div>

<div class="container-fluid bg-3">
	<div class="row">
		<div class="col-sm-1"></div>
		<div class="col-sm-10">
			<div class="Evaluation">
								<h2>Evaluation</h2>

		<p>The evaluation of machine translation systems is crucial for assessing their performance, and the BLEU score is commonly used as the standard baseline metric. In addition to BLEU, other metrics like TER and CHRFF are also employed.</p>
				<p>At the outset, participants will receive a eval dataset to evaluate their models. They can employ the provided test data to assess their system's performance and upload the results. This allows participants to view their performance on a leaderboard, where rankings are determined based on the evaluation results.</p>
				<p>Subsequently, participants are required to submit the predicted sentences for another dataset referred to as the "golden test data." These predicted sentences should be included in a file named test-pred.(lang-code), where "lang-code" represents the language code associated with the translation.</p>
				<p>This comprehensive evaluation process enables participants to gauge the effectiveness of their machine translation systems by leveraging widely accepted metrics such as BLEU, TER, and METEOR.</p>
				<p>Following are the Evaluation Matrices: </p>
				<ol>
					<li> CHRF++ : ChrF and ChrF++ are two MT evaluation metrics. They both use the F-score statistic for character n-gram matches, and ChrF++ adds word n-grams as well which correlates more strongly with direct assessment. </li>
					<li>BLEU :  a metric for automatically evaluating machine-translated text. The BLEU score is a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations.</li>
					<li>TER : TER (Translation Edit Rate, also called Translation Error Rate) is a metric to quantify the edit operations that a hypothesis requires to match a reference translation. </li>
				</ol>
				<p>However teams will be ranked based on CHRF++ scores.<br> For simulating the evaluation SacreBLEU library can be used.
					<a href="https://huggingface.co/spaces/evaluate-metric/sacrebleu" >Use this Link</a></p>
				
				
				<!-- <br>In Task 2 the F1 score of individual arguments will be taken into consideration and then it will be averaged out. The strings that are an exact match will only be considered as True.<br>
For example: If the PLACE argument in Test data is New Delhi and the output of the PLACE argument for test data by the participant&rsquo;s method is Delhi then it won&rsquo;t be considered a match.<br>
The evaluation of all the five languages will be done separately.</p>
 -->				<br>
			</div>
		</div>
		
	</div>

	</div>
</div>
<br>
<br>

<footer class="sticky-footer">
	<div class="container my-auto">
		<div class="copyright text-center my-auto">
					<span>Copyright Â© MTIL 2023</span>
		</div>
	</div>
</footer>
<script
  src="https://code.jquery.com/jquery-3.4.1.min.js"
  integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
  crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.1/umd/popper.min.js" integrity="sha256-/ijcOLwFf26xEYAjW75FizKVo5tnTYiQddPZoLUHHZ8=" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

</body>
</html>
